# Here are scripts and info used to generate Figure 1 and Supplemental Fig S1
	# Figure 1 is based on 70 traits found in 'TraitsDirectories.Astle'
	# Supplemental Figure S1 is based on 45 traits found in 'TraitsDirectories.Astle.Pruned_r0.8'

# HyPrColoc script
  ##!usr/bin/python

  ##this will create matrices of ChrBP x Traits for a bunch of GWAS, to be loaded into HyPrColoc for multitrait analysis
  ##Need a matrix for beta effects, and another one for Std Errors


  ##the command I used for training is python HyPrCOLOC.MakeMatrices.py TraitsDirectories.PltRBCWBC
  ##the command to run this for all Astle SNPs is python HyPrCOLOC.MakeMatrices.py TraitsDirectories
import sys
import subprocess
import pandas as pd
import numpy as np
import os
import glob

if len(sys.argv) != 3 :
    print("\nUsage: python HyPrCOLOC.MakeMatrics.MarkerName.py <List of Heme Traits to Coloc (should be a list with Trait -tab- Directory.gz)> <SNP file
 list *make sure the .py script has the correct columns noted*>\n\n Note that this makes a Chr:Pos for each snp rather than going by rsid\nThis is also 
dependent on a shell script FixMatrices.sh \n\n")
    exit()
    
traitsdirs = sys.argv[1] # eg /project/voight_ML/thomc/thomc_results/COLOC/TraitsDirectories.PltRBCWBC ###THIS IS WHERE I WILL SPECIFY ALL THE DIRECTORI
ES ONCE PAST TESTING PHASE####
snpfile = sys.argv[2]

###########################
##this will make genome-wide Beta Effect matrix that needs to be parsed down to GWAS signal regions later
############################
if not os.path.isfile('BetaMatrix.Genome.csv'):
    betas = pd.DataFrame(columns=['chr:pos', 'chr', 'pos'])
    ses = pd.DataFrame(columns=['chr:pos', 'chr', 'pos'])
    
    td = open(traitsdirs)
    for line in td:
        line = line.strip()
        sl = line.split('\t')
        Trait = sl[0]
        file = sl[1]
        
        if '.gz' in file:
            f = pd.read_csv(file, compression='gzip', header=0, delim_whitespace=True)
            f.columns = f.columns.str.lower()
            print(Trait)
            f.rename(columns={'hg19_pos':'pos', 'chromosome':'chr', 'position':'pos', 'bp':'pos', 'chrom':'chr', 'chromend':'pos'}, inplace=True)
            f.rename(columns={'effect':'beta', 'or':'beta', 'bcac_gwas_all_beta':'beta', 'european_ancestry_beta_fix':'beta'}, inplace=True)
            f.rename(columns={'stderr':'se', 'bcac_gwas_all_se':'se', 'european_ancestry_se_fix':'se'}, inplace=True)
            #print(f.head())
            f['chr:pos'] = f['chr'].astype(str) + ":" + f['pos'].astype(str)   #].apply(lambda x: ':'.join(x), axis=1) #f['chr'] + ":" + f['pos']   #f.a
pply(lambda row: row.a + row.b, axis=1)
            f = f.drop_duplicates(['chr:pos']) #default is to keep the 'first' duplicate. this is arbitrary here
            
            #####Make Beta matrix
            b = f[['chr:pos', 'beta']].copy(deep=True)
            b.rename(columns={'beta':Trait}, inplace=True)
            print("Length of b index = "+str(len(b.index)))
            if (len(betas.index) == 0): #this 'initializes' betas dataframe with all rows from first file
                betas = f[['chr:pos', 'chr', 'pos']]
            
            betas = pd.merge(left=betas, right=b, on='chr:pos')
            print("Length of betas = "+str(len(betas.index)))
            
            ######Make SE matrix
            s = f[['chr:pos', 'se']].copy(deep=True)
            s.rename(columns={'se':Trait}, inplace=True)
            if (len(ses.index) == 0): #initializes dataframe if it's first feature
                ses = f[['chr:pos', 'chr', 'pos']]
            
            ses = pd.merge(left=ses, right=s, on='chr:pos')
            print("Length of ses index = "+str(len(ses.index)))
            
            export_csv = betas.to_csv(r'BetaMatrix.Genome.csv', index = False, header=True) #Don't forget to add '.csv' at the end of the path
            print(betas.head())
    
            export_csv = ses.to_csv(r'SEsMatrix.Genome.csv', index = False, header=True) #Don't forget to add '.csv' at the end of the path
            print(ses.head())

        else:
            f = pd.read_csv(file, header=0, delim_whitespace=True)
            f.columns = f.columns.str.lower()
            print(Trait)
            f.rename(columns={'hg19_pos':'pos', 'chromosome':'chr', 'position':'pos', 'bp':'pos', 'chrom':'chr', 'chromend':'pos'}, inplace=True)
            f.rename(columns={'effect':'beta', 'or':'beta', 'bcac_gwas_all_beta':'beta', 'european_ancestry_beta_fix':'beta'}, inplace=True)
            f.rename(columns={'stderr':'se', 'bcac_gwas_all_se':'se', 'european_ancestry_se_fix':'se'}, inplace=True)

            f['chr:pos'] = f['chr'].astype(str) + ":" + f['pos'].astype(str)   #].apply(lambda x: ':'.join(x), axis=1) #f['chr'] + ":" + f['pos']   #f.a
pply(lambda row: row.a + row.b, axis=1)
            f = f.drop_duplicates(['chr:pos']) #default is to keep the 'first' duplicate. this is arbitrary here
            
            #####Make Beta matrix
            b = f[['chr:pos', 'beta']].copy(deep=True)
            b.rename(columns={'beta':Trait}, inplace=True)
            print("Length of b index = "+str(len(b.index)))
            if (len(betas.index) == 0): #this 'initializes' betas dataframe with all rows from first file
                betas = f[['chr:pos', 'chr', 'pos']]
            
            betas = pd.merge(left=betas, right=b, on='chr:pos')
            print("Length of betas = "+str(len(betas.index)))
            
            ######Make SE matrix
            s = f[['chr:pos', 'se']].copy(deep=True)
            s.rename(columns={'se':Trait}, inplace=True)
            if (len(ses.index) == 0): #initializes dataframe if it's first feature
                ses = f[['chr:pos', 'chr', 'pos']]
            
            ses = pd.merge(left=ses, right=s, on='chr:pos')
            print("Length of ses index = "+str(len(ses.index)))
            
            export_csv = betas.to_csv(r'BetaMatrix.Genome.csv', index = False, header=True) #Don't forget to add '.csv' at the end of the path
            print(betas.head())
    
            export_csv = ses.to_csv(r'SEsMatrix.Genome.csv', index = False, header=True) #Don't forget to add '.csv' at the end of the path
            print(ses.head())

    td.close()

    
############################
####Make a smaller subset of regions based on GWAS significant loci that is able to go into HyPrColoc

betas = pd.read_csv('BetaMatrix.Genome.csv')
betas.rename(columns = {'chr:pos':'SNP'}, inplace = True)

ses = pd.read_csv('SEsMatrix.Genome.csv')
ses.rename(columns = {'chr:pos':'SNP'}, inplace = True)

with open("Output.HyPrColoc", "w") as f:
    f.write("iteration\ttraits\tposterior_prob\tregional_prob\tcandidate_snp\tposterior_explained_by_snp\tdropped_trait\n")
f.close()

snps = open('AstleSNPs')

for line in open(snpfile):
    sl = line.split('\t')
    chrom = int(sl[0])
    loc = int(sl[2])
    ub = loc+250000
    lb = loc-250000

    subbetas = betas.loc[(betas['chr'] == chrom) & (betas['pos'] <= ub) & (betas['pos'] >= lb)]
    BetaFile = open("sub.BetaMatrix.csv", 'w')
    subbetas.to_csv(BetaFile, index=False, sep="\t")

    subses = ses.loc[(ses['chr'] == chrom) & (ses['pos'] <= ub) & (ses['pos'] >= lb)]
    SesFile = open("sub.SEsMatrix.csv", 'w')
    subses.to_csv(SesFile, index=False, sep="\t")

    BetaFile.close()
    SesFile.close()

    cmd = "Rscript HyPrCOLOC.Input.R"
    subprocess.call(cmd, shell=True)

exit()
=====================================================
=====================================================

#!usr/bin/python
#GWAS.QTLsearch.py created by CST on 181203 to identify overlaps between a list of SNPs (e.g. those from a GWAS, like the MVP.T2DM GWAS) and QTL databases (paths to which will be specified in input)

import sys
from io import StringIO
import os
import re
import pdb
import glob #makes opening files easier with wildcards
import mmap #this helps to not have to open a bunch of files, saves as strings instead (??)
import subprocess
from collections import Counter
import time
import pandas as pd
import numpy as np

############
##this will start from an rsID list generated by Coloc or HyPrColoc...but I want to find all the highly linked SNPs too to ensure complete identification of related QTLs
# so will use plink to identify linked r2>0.9 snps to the coloc snps
############


###########
####example command line: 
# python /project/voight_ML/thomc/thomc_scripts/thomc_py_scripts/GWAS.QTLsearch.py /project/voight_ML/thomc/thomc_results/HyPrCOLOC/Astle.2704snps.HemeCAD /project/voight_ML/thomc/thomc_results/HyPrCOLOC/Astle.2704snps.HemeCAD/
Astle.2704snps.HemeCAD.Output.HyPrColoc 0.9 

## the following directories are within the shell script called in this .py
# eQTL directory is /project/voight_datasets/GTEx_v7/GTEx_Analysis_v7_eQTL/ 
# sQTL directory is /project/voight_datasets/GTEx_V3Pilot/sQTL/ 
# vQTL directory is /project/voight_datasets/annot/vQTL/ GWAS.QTLsearch.Output

##note this command does NOT include Altrans sQTLs nor Kat's sQTLs 
###########

#First do the basic stuff - load plink and make a directory to store everything
#plink = "module load plink"

#Define arguments - will need GWAS SNP file, eQTL files, sQTL files, vQTL file (at minimum)
if len(sys.argv)!= 5:
    print("\n\nRun Fail\nUsage: GWAS.QTLsearch.py by CST\nNeed to input:\n\npython <directions to GWAS.QTLsearch.py, maybe within thomc_py_scripts> </full/path/ working directory (e.g. pwd)> </full/path/ to Beta File> </full pa
th/ Output.HyPrColoc file - this will find unique rsIDs within that and input into plink, etc> < r2 value for linked snps, prob 0.99> \n\n these are not necessary anymore: <eQTL file directory?> <sQTLs?> <vQTLs?> <Output Direct
ory.\n\nPut wd in WITHOUT the / at the end\n\nResult should be output in a new directory PLINK within your set wd\n\n")
    exit()

wd = sys.argv[1]
betafile = sys.argv[2]
rsidfile = sys.argv[3]
r2 = sys.argv[4] 
#eqtl = sys.argv[4]
#sqtl = sys.argv[5]
#vqtl = sys.argv[6]
#outFiles = sys.argv[7]

#take HyPrColoc results and find the unique rsIDs that were identified  ####if you want just a subet of Traits, then grep those out separately and change the Output.HyPrColoc file path on command line (e.g. plt && rbc)###
if not os.path.isfile('rsids'):
    rsIDs = "awk -F \'\\t\' \'{print $5}\' " +rsidfile+ " | sort -u | grep \'rs\' > rsids" 
    subprocess.call(rsIDs, shell=True)
    rsids = open('rsids', 'r')

else: 
    print("working from pre-tabulated rsids file")
    rsids = open('rsids', 'r')
    rsids.columns = ['SNP', 'RSID'] #SNP is in the form chr:pos 


#print("unique rsid wc is: ")
#wc = "awk -F \'\\t\' \'{print $5}\' " +rsidfile+ " | sort -u | grep \'rs\' | wc"
#subprocess.call(wc, shell=True)

#then plink into chromosomes -- this uses 'rsids' as the snp list, as defined above  #####if this is failing you probably forgot to module load plink!!!
f = pd.read_csv(betafile, header=0)

for snp in rsids:
    snp = snp.strip()
    sl = snp.split("\t")
    
    subprocess.call("mkdir " + wd + "/plink", shell=True)
    currsnp = open("currsnp","w")
    #########change this if there's only a list of rsIDs!! snp['RSID'] refers to when rsids file has CHR POS RSID columns
    currsnp.write(sl[1])
    #currsnp.write(snp)
    currsnp.close()
    print(f.columns)
    
    print("Currently analyzing: "+str(snp))
    snpdata = f[(f['SNP'].str.endswith(sl[0])) & (f['SNP'].str.startswith(sl[0]))].copy(deep=True) #this is used if rsid file has chr:pos \t rsid format.... use next line if it's  just a list of rsids
    snpdata['SNP'] = sl[1] # change chrpos to rsid
    #snpdata = f[f['SNP'].str.endswith(snp)].copy(deep=True) #https://stackoverflow.com/questions/20625582/how-to-deal-with-settingwithcopywarning-in-pandas
    
    #cols = list(snpdata.columns) #this is how you would reorder columns, moving last column to first
    #cols = [cols[-1]] + cols[:-1]
    #snpdata = snpdata[cols]
    
    print(snpdata)

    #print(snpdata.CHR.item())
    chr = snpdata.chr.item()
    #print(chr)
    pos = snpdata.pos.item()
    #print(pos)
        
    bsub = "plink --allow-no-sex --bfile /project/voight_datasets_01/1kg/phaseIII_2013/plink_files/chr" + str(chr) + " --keep /project/voight_ML/lorenzk/all_EUR_1kg.txt --exclude /project/voight_ML/lorenzk/1kg_duplicated_rsIDs.
txt --r2 --ld-snp-list currsnp --ld-window-r2 " +r2 + " --ld-window 99999 --ld-window-kb 250000 --out " + wd + "/plink/chr" + str(chr)
    #print(bsub)

    #    for i in range(1,23):
    #       bsub = "bsub -q voight_normal plink --allow-no-sex --bfile /project/voight_datasets/1kg/phaseIII_2013/plink_files/chr" + str(i) + " --keep /project/voight_ML/lorenzk/all_EUR_1kg.txt --exclude /project/voight_ML/lore
nzk/1kg_duplicated_rsIDs.txt --r2 --ld-snp-list rsids --ld-window-r2 " +r2 + " --ld-window 99999 --ld-window-kb 250000 --out " + wd + "/plink/chr" + str(i)
    subprocess.call(bsub, shell=True)
    
    #print ("Running\n" +snp+ "\n")
    
    #this is a shell script that will grep for eQTLs.... can also include sQTL and vQTL stuff later!
    subprocess.call("/project/voight_ML/thomc/thomc_scripts/thomc_py_scripts/qtlsearch.sh", shell=True)

    print("the number of unique snps that are eQTLs is:")
    subprocess.call("awk \'{print $1}\' eQTLs.r8.txt.ForTable | sort -u | wc", shell=True)
    print("The number of genes impacted by these eQTLs is")
    subprocess.call("awk \'{print $2}\' eQTLs.r8.txt.ForTable | sed \'s/\./\\t/\' | awk \'{print $1}\' | sort -u | wc", shell=True)

    with open('nearestgene', 'r') as file:
        data = file.read().replace('\n', ', ')
        data = data.rstrip(', ')
        snpdata['NearestGenes'] = data
            
    with open('ensgs', 'r') as file:
        data = file.read().replace('\n', ', ')
        data = data.rstrip(', ')
        snpdata['eQTL_GeneNames'] = data

    with open('ensgs.blood', 'r') as file:
        data = file.read().replace('\n', ', ')
        data = data.rstrip(', ')
        snpdata['eQTL_GeneNames_blood'] = str(data)
        print(snpdata)                                                                                                                                                                                                             
                           
    with open('sQTLSeekergenes', 'r') as file:
        data = file.read().replace('\n', ', ')
        data = data.rstrip(', ')
        snpdata['sQTL_Seeker_GeneNames'] = data

    with open('sQTLAltransgenes', 'r') as file:
        data = file.read().replace('\n', ', ')
        data = data.rstrip(', ')
        snpdata['sQTL_Altrans_GeneNames'] = data

    with open('sQTLKat', 'r') as file:
        data = file.read().replace('\n', ', ')
        data = data.rstrip(', ')
        snpdata['sQTL_Kat_GeneNames'] = data

    with open('vQTL.Genes', 'r') as file:
        data = file.read().replace('\n', ', ')
        data = data.rstrip(', ')
        snpdata['vQTL_GeneNames'] = data
        print(snpdata)
        header = list(snpdata)

    #Print out a table that includes rsid chr pos traits betas ses eQTL_ENSG --- note this is without headers
    snpdata.to_csv(r'tmp.csv', mode='a', index = False, header=False, sep="\t")
    subprocess.call("rm -r plink/", shell=True)

#get tmp.csv in sorted order
subprocess.call("sort -nk2 -nk3 tmp.csv > tmp", shell=True)
subprocess.call("mv tmp tmp.csv", shell=True)
        
#Add headers to output
df = pd.read_csv('tmp.csv', header=None, sep="\t")
df.to_csv(r'QTLsearch.txt', mode='w', index=False, header=header, sep="\t")
subprocess.call("rm tmp.csv", shell=True)

#df = pd.read_csv('QTLsearch.txt', sep="\t")
#print(df.columns)
#print(df.dtypes)
#df.to_csv(r'QTLsearch.MPV.CAD.csv', columns=['SNP', 'chr', 'pos', 'cad', 'mpv', 'NearestGenes', 'eQTL_GeneNames', 'eQTL_GeneNames_blood', 'sQTL_Seeker_GeneNames', 'sQTL_Altrans_GeneNames', 'sQTL_Kat_GeneNames', 'vQTL_GeneNames
'], index=False, sep="\t")
#df.to_csv(r'QTLsearch.HSC.csv', columns=['SNP', 'chr', 'pos', 'rbc', 'plt', 'wbc', 'NearestGenes', 'eQTL_GeneNames', 'eQTL_GeneNames_blood', 'sQTL_Seeker_GeneNames', 'sQTL_Altrans_GeneNames', 'sQTL_Kat_GeneNames', 'vQTL_GeneNa
mes'], index=False, sep="\t")

subprocess.call("rm currsnp", shell=True)
subprocess.call("rm ensgs", shell=True)
subprocess.call("rm ensgs.blood", shell=True)
subprocess.call("rm eQTL.Genes", shell=True)
subprocess.call("rm eQTL.Genes.blood", shell=True)
subprocess.call("rm eQTLs.r8.txt", shell=True)
subprocess.call("rm eQTLs.r8.txt.ForTable", shell=True)
subprocess.call("rm nearestgene", shell=True)
subprocess.call("rm sQTLAltransgenes", shell=True)
subprocess.call("rm sQTLKat", shell=True)
subprocess.call("rm sQTLSeekergenes", shell=True)
subprocess.call("rm sQTLs.r8.txt", shell=True)
subprocess.call("rm vQTL.Genes", shell=True)

=====================================================
=====================================================

#TraitMatchQTLs.HyPrColoc.Output.py
#!usr/bin/python

#this will take a file of QTLs (Output.HyPrColoc and /project/voight_ML/thomc/thomc_scripts/thomc_py_scripts/GWAS.QTLsearch.py outputs), grep for certain traits or circumstances of interest, and output tables based on hard code
d columns --> applicable to make Tables for paper)

import sys
import subprocess
import pandas as pd
import numpy as np
import os
import glob

if len(sys.argv) != 3 :
    print("\n\nUsage: python TraitMatchQTLs.HyPrColoc.Output.py <...Output.HyPrColoc file> <QTLsearch.txt or equivalent file>\n\nWill need to hard-code in any trait combinations you'd like!\n\nNote that this is dependent on a s
hell script LocusSearch.sh which should make all the outputs you want. Will need to add to both shell script and this py script if you want new trait combo\nMake sure the Output.HyPrColoc file you input here is the same that is
 hard-coded into LocusSearch.sh\n\n")
    exit()
    
hypr = sys.argv[1] # eg AstleSNPs.AstleTraits.Output.HyPrColoc
qtls = sys.argv[2] # eg ../HyPrColoc/Astle.2704snps.74traits/Astle2704snps.MPV.CAD/QTLsearch.csv

f = pd.read_csv(qtls, header=0, sep="\t")

##############
#grep for all the combinations you want first to make new search files (e.g. grep 'mpv' and 'cad' > mpv.cad.snps  ... will look like ,chr,pos, so that can get exact match later)
subprocess.call("sh /project/voight_ML/thomc/thomc_scripts/thomc_py_scripts/LocusSearch.sh " + hypr, shell=True)

# then for each file will search in pandas and print out a table with just SNP, chr, pos, [traits], QTL info
#Want HSC, MEP, Plt (no RBC or WBC), RBC (no Plt or WBC) from AstleSNPs/Traits

#Want 'lots of traits',mpv.cad, ret.cad, ret.t2d, etc ... from 70 harmonized traits coloc

#HSC
header = ['SNP', 'chr', 'pos', 'wbc', 'plt', 'rbc', 'NearestGenes', 'eQTL_GeneNames', 'eQTL_GeneNames_blood', 'sQTL_Seeker_GeneNames', 'sQTL_Altrans_GeneNames', 'sQTL_Kat_GeneNames', 'vQTL_GeneNames']
subprocess.call("rm HSC.txt", shell=True)

for line in open('HSC.loci', 'r'):
    line.strip()
    sl = line.split(":")
    sl[1] = sl[1].rstrip('\n')
    #print("-"+sl[0]+"-")
    #print("-"+sl[1]+"-")
    d = f[ (f['chr'].astype(str)==sl[0]) & (f['pos'].astype(str)==sl[1]) ].copy(deep=True)
    if not os.path.isfile('HSC.txt'): #only print header once
        d.to_csv(r'HSC.txt', mode='w', columns=header, index=False, header=True, sep="\t", na_rep="")
    else:
        d.to_csv(r'HSC.txt', mode='a', columns=header, index=False, header=False, sep="\t", na_rep="")

#MEP
header = ['SNP', 'chr', 'pos', 'plt', 'rbc', 'NearestGenes', 'eQTL_GeneNames', 'eQTL_GeneNames_blood', 'sQTL_Seeker_GeneNames', 'sQTL_Altrans_GeneNames', 'sQTL_Kat_GeneNames', 'vQTL_GeneNames']
subprocess.call("rm MEP.txt", shell=True)

for line in open('MEP.loci', 'r'):
    line.strip()
    sl = line.split(":")
    sl[1] = sl[1].rstrip('\n')
    d = f[ (f['chr'].astype(str)==sl[0]) & (f['pos'].astype(str)==sl[1]) ].copy(deep=True)
    if not os.path.isfile('MEP.txt'): #only print header once
        d.to_csv(r'MEP.txt', mode='w', columns=header, index=False, header=True, sep="\t", na_rep="")
    else:
        d.to_csv(r'MEP.txt', mode='a', columns=header, index=False, header=False, sep="\t", na_rep="")


#Platelet only
header = ['SNP', 'chr', 'pos', 'plt', 'pct', 'mpv', 'pdw', 'NearestGenes', 'eQTL_GeneNames', 'eQTL_GeneNames_blood', 'sQTL_Seeker_GeneNames', 'sQTL_Altrans_GeneNames', 'sQTL_Kat_GeneNames', 'vQTL_GeneNames']
subprocess.call("rm Plt.txt", shell=True)

for line in open('Plt.loci', 'r'):
    line.strip()
    sl = line.split(":")
    sl[1] = sl[1].rstrip('\n')
    d = f[ (f['chr'].astype(str)==sl[0]) & (f['pos'].astype(str)==sl[1]) ].copy(deep=True)
    if not os.path.isfile('Plt.txt'): #only print header once
        d.to_csv(r'Plt.txt', mode='w', columns=header, index=False, header=True, sep="\t", na_rep="")
    else:
        d.to_csv(r'Plt.txt', mode='a', columns=header, index=False, header=False, sep="\t", na_rep="")

#Platelet count specifically
header = ['SNP', 'chr', 'pos', 'plt', 'pct', 'mpv', 'pdw', 'NearestGenes', 'eQTL_GeneNames', 'eQTL_GeneNames_blood', 'sQTL_Seeker_GeneNames', 'sQTL_Altrans_GeneNames', 'sQTL_Kat_GeneNames', 'vQTL_GeneNames']
subprocess.call("rm PltCount.txt", shell=True)

for line in open('PltCount.loci', 'r'):
    line.strip()
    sl = line.split(":")
    sl[1] = sl[1].rstrip('\n')
    d = f[ (f['chr'].astype(str)==sl[0]) & (f['pos'].astype(str)==sl[1]) ].copy(deep=True)
    if not os.path.isfile('PltCount.txt'): #only print header once
        d.to_csv(r'PltCount.txt', mode='w', columns=header, index=False, header=True, sep="\t", na_rep="")
    else:
        d.to_csv(r'PltCount.txt', mode='a', columns=header, index=False, header=False, sep="\t", na_rep="")


#Red cell only
header = ['SNP', 'chr', 'pos', 'rbc', 'hct', 'mcv', 'rdw', 'NearestGenes', 'eQTL_GeneNames', 'eQTL_GeneNames_blood', 'sQTL_Seeker_GeneNames', 'sQTL_Altrans_GeneNames', 'sQTL_Kat_GeneNames', 'vQTL_GeneNames']
subprocess.call("rm Rbc.txt", shell=True)

for line in open('Rbc.loci', 'r'):
    line.strip()
    sl = line.split(":")
    sl[1] = sl[1].rstrip('\n')
    d = f[ (f['chr'].astype(str)==sl[0]) & (f['pos'].astype(str)==sl[1]) ].copy(deep=True)
    if not os.path.isfile('Rbc.txt'): #only print header once
        d.to_csv(r'Rbc.txt', mode='w', columns=header, index=False, header=True, sep="\t", na_rep="")
    else:
        d.to_csv(r'Rbc.txt', mode='a', columns=header, index=False, header=False, sep="\t", na_rep="")

#Rbc count specifically
header = ['SNP', 'chr', 'pos', 'rbc', 'hct', 'mcv', 'rdw', 'NearestGenes', 'eQTL_GeneNames', 'eQTL_GeneNames_blood', 'sQTL_Seeker_GeneNames', 'sQTL_Altrans_GeneNames', 'sQTL_Kat_GeneNames', 'vQTL_GeneNames']
subprocess.call("rm RbcCount.txt", shell=True)

for line in open('RbcCount.loci', 'r'):
    line.strip()
    sl = line.split(":")
    sl[1] = sl[1].rstrip('\n')
    d = f[ (f['chr'].astype(str)==sl[0]) & (f['pos'].astype(str)==sl[1]) ].copy(deep=True)
    if not os.path.isfile('RbcCount.txt'): #only print header once
        d.to_csv(r'RbcCount.txt', mode='w', columns=header, index=False, header=True, sep="\t", na_rep="")
    else:
        d.to_csv(r'RbcCount.txt', mode='a', columns=header, index=False, header=False, sep="\t", na_rep="")


#Iron, Plt, red cell stuff 
header = ['SNP', 'chr', 'pos', 'plt', 'mpv', 'rbc', 'NearestGenes', 'eQTL_GeneNames', 'eQTL_GeneNames_blood', 'sQTL_Seeker_GeneNames', 'sQTL_Altrans_GeneNames', 'sQTL_Kat_GeneNames', 'vQTL_GeneNames']
subprocess.call("rm Iron.txt", shell=True)

for line in open('Iron.loci', 'r'):
    line.strip()
    sl = line.split(":")
    sl[1] = sl[1].rstrip('\n')
    d = f[ (f['chr'].astype(str)==sl[0]) & (f['pos'].astype(str)==sl[1]) ].copy(deep=True)
    if not os.path.isfile('Iron.txt'): #only print header once
        d.to_csv(r'Iron.txt', mode='w', columns=header, index=False, header=True, sep="\t", na_rep="")
    else:
        d.to_csv(r'Iron.txt', mode='a', columns=header, index=False, header=False, sep="\t", na_rep="")

#LotsTraits (>= 20 coloc at single locus)
header = ['SNP', 'chr', 'pos', 'NearestGenes', 'eQTL_GeneNames', 'eQTL_GeneNames_blood', 'sQTL_Seeker_GeneNames', 'sQTL_Altrans_GeneNames', 'sQTL_Kat_GeneNames', 'vQTL_GeneNames']
subprocess.call("rm LotsTraits.txt", shell=True)

for line in open('LotsTraits.loci', 'r'):
    line.strip()
    sl = line.split(":")
    sl[1] = sl[1].rstrip('\n')
    d = f[ (f['chr'].astype(str)==sl[0]) & (f['pos'].astype(str)==sl[1]) ].copy(deep=True)
    if not os.path.isfile('LotsTraits.txt'): #only print header once
        d.to_csv(r'LotsTraits.txt', mode='w', columns=header, index=False, header=True, sep="\t", na_rep="")
    else:
        d.to_csv(r'LotsTraits.txt', mode='a', columns=header, index=False, header=False, sep="\t", na_rep="")
	
# etc... for all trait combinations that are interesting!

=====================================================
=====================================================

#qtlsearch.sh
awk '{print $4"_"$5"_\t"$6"\t"$4"\t"$5}' plink/*.ld > plink/rsid_LD
grep -v ";" plink/rsid_LD > plink/tmp
mv plink/tmp plink/rsid_LD
sed -i 's/_B//g' plink/rsid_LD
sed -i '/CHR/d' plink/rsid_LD
awk '{print $1}' plink/rsid_LD | sort -u > plink/rsid_LD.chr_bp_
awk '{print $2}' plink/rsid_LD | sort -u | grep -v "esv" | sed 's/\;/\n/g' > plink/rsid_LD.rsID
awk '{print "chr"$3"\t"$4"\t"$4+1}' plink/rsid_LD | sort -u > plink/rsid_LD.bed

echo 'made rsid files'

closestBed -a plink/rsid_LD.bed -b /project/voight_selscan/ksiewert/BetaGenomeScan/genes_biomart_sorted.bed -d -t first > neargene.bed
awk '{print $7}' neargene.bed | sort -u > nearestgene

echo 'done closestbed'

grep -Ff plink/rsid_LD.chr_bp_ /project/voight_datasets/GTEx_v7/GTEx_Analysis_v7_eQTL/*signif_variant_gene_pairs.txt > eQTLs.r8.txt
sed -i 's/\/project\/voight_datasets\/GTEx_v7\/GTEx_Analysis_v7_eQTL\///' eQTLs.r8.txt
sed -i 's/.v7.signif_variant_gene_pairs.txt//' eQTLs.r8.txt
sed -i 's/\:/\t/' eQTLs.r8.txt
sort -k2 eQTLs.r8.txt | awk '{print $2"\t"$3"\t"$1}' > eQTLs.r8.txt.ForTable
awk '{print $2}' eQTLs.r8.txt.ForTable | sed 's/\./\t/' | awk '{print $1}' | sort -u > tmp
grep 'Whole_Blood' eQTLs.r8.txt.ForTable | awk '{print $2}' | sed 's/\./\t/' | awk '{print $1}' | sort -u > tmp.blood
grep -Fwf tmp /project/voight_ML/thomc/thomc_results/COLOC/hg19_GeneNames_ENSG > eQTL.Genes
grep -Fwf tmp.blood /project/voight_ML/thomc/thomc_results/COLOC/hg19_GeneNames_ENSG > eQTL.Genes.blood
awk '{print $1}' eQTL.Genes > ensgs
awk '{print $1}' eQTL.Genes.blood > ensgs.blood

echo 'done eqtls'

grep -Ff plink/rsid_LD.rsID /project/voight_datasets/GTEx_V3Pilot/sQTL/sQTLs-* > sQTLs.r8.txt
sed -i 's/\:/\t/' sQTLs.r8.txt
grep -o "ENSG[0-9]*" sQTLs.r8.txt | sort -u > sQTL.ENSG
grep -Ff sQTL.ENSG /project/voight_datasets/annot/hg19_genes_ENSG.txt > sQTL.GeneNames
awk '{print $4}' sQTL.GeneNames > sQTLSeekergenes

grep -Ff plink/rsid_LD.rsID /project/voight_datasets/GTEx_V3Pilot/sQTL/Altrans_All_sQTLs_bestPerLink/*Altrans.bestPerLink | awk '($6+0) > 6' > sQTL.altrans
sed -i 's/\:/\t/' sQTL.altrans
grep -o "ENSG[0-9]*" sQTL.altrans | sort -u > sQTL.altrans.ensg
grep -Ff sQTL.altrans.ensg /project/voight_datasets/annot/hg19_genes_ENSG.txt > sQTL.altrans.genes
awk '{print $4}' sQTL.altrans.genes > sQTLAltransgenes

grep -Ff plink/rsid_LD.rsID /project/voight_ML/thomc/thomc_results/MVP.T2DM.GWAS/Kat.sQTL/* > sQTLKat

echo 'done sqtls'

grep -Ff plink/rsid_LD.rsID /project/voight_datasets/annot/vQTL/eLife.vQTL.1Afortxt.txt > vQTL.1A
awk '{print $2}' vQTL.1A > vQTL.1A.Genes

grep -Ff plink/rsid_LD.rsID /project/voight_datasets/annot/vQTL/eLife.vQTL.1Bfortxt.txt > vQTL.1B
awk '{print $2}' vQTL.1B > vQTL.1B.Genes
cat vQTL.1A.Genes vQTL.1B.Genes | sort -u > vQTL.Genes

echo 'done vqtls'

rm vQTL.1A
rm vQTL.1B
rm vQTL.1A.Genes
rm vQTL.1B.Genes
rm sQTL.altrans.genes
rm sQTL.altrans
rm sQTL.GeneNames
rm sQTL.ENSG
rm neargene.bed
rm tmp
rm tmp.blood

=====================================================
=====================================================
#LocusSearch.sh

grep 'wbc' $1 | grep 'plt' | grep 'rbc' | awk -F "\t" '{print $5}' | sort -u > HSC.loci

egrep -v 'wbc|gran|neut|eo|lymph|mono|baso' $1 | grep 'plt' | grep 'rbc' | awk -F "\t" '{print $5}' | sort -u > MEP.loci

egrep -v 'wbc|gran|neut|eo|lymph|mono|baso' $1 | egrep 'plt|pct|pdw|mpv' | egrep -v 'rbc|hct|mcv|mch|rdw' | awk -F "\t" '{print $5}' | sort -u > Plt.loci

egrep -v 'wbc|gran|neut|eo|lymph|mono|baso' $1 | grep 'plt' | egrep -v 'rbc|hct|mcv|mch|rdw' | awk -F "\t" '{print $5}' | sort -u > PltCount.loci

egrep -v 'wbc|gran|neut|eo|lymph|mono|baso' $1 | egrep -v 'plt|pct|pdw|mpv' | egrep 'rbc|hct|mcv|mch|rdw' | awk -F "\t" '{print $5}' | sort -u > Rbc.loci

egrep -v 'wbc|gran|neut|eo|lymph|mono|baso' $1 | egrep -v 'plt|pct|pdw|mpv' | grep 'rbc' | awk -F "\t" '{print $5}' | sort -u > RbcCount.loci

egrep -v 'wbc|gran|neut|eo|lymph|mono|baso' $1 | grep 'plt|pct' | egrep 'rbc|mcv' | awk -F "\t" '{print $5}' | sort -u > Iron.loci




awk 'NF > 15' $1 | awk -F "\t" '{print $5}' | sort -u > LotsTraits.loci

grep 'mpv' $1 | grep 'cad' | awk -F "\t" '{print $5}' | sort -u > MpvCad.loci

grep 'ret' $1 | grep 'cad' | awk -F "\t" '{print $5}' | sort -u > RetCad.loci

grep 'ret' $1 | grep 't2d' | awk -F "\t" '{print $5}' | sort -u > RetT2d.loci

grep 'lymph' $1 | grep 'cad' | awk -F "\t" '{print $5}' | sort -u > LymphCad.loci

grep 'lymph' $1 | grep 'SCZ' | awk -F "\t" '{print $5}' | sort -u > LymphScz.loci

grep 'lymph' $1 | grep 'Asthma' | awk -F "\t" '{print $5}' | sort -u > LymphAsthma.loci

grep 'mono' $1 | grep 'Asthma' | awk -F "\t" '{print $5}' | sort -u > MonoAsthma.loci

egrep 'eo|eo_p' $1 | grep 'Asthma' | awk -F "\t" '{print $5}' | sort -u > EoAsthma.loci

grep 'neut' $1 | grep 'Asthma' | awk -F "\t" '{print $5}' | sort -u > NeutAsthma.loci

egrep 'eo|eo_p' $1 | grep 'Migraine' | awk -F "\t" '{print $5}' | sort -u > EoMigraine.loci

grep 'CLL' $1 | egrep 'ret|irf|hlr_p' | awk -F "\t" '{print $5}' | sort -u > CllRedcell.loci

grep 'UKBB_chf' $1 | egrep 'ibd|uc' | awk -F "\t" '{print $5}' | sort -u > ChfIbd.loci

grep 't2d' $1 | grep 'ProstateCa' | awk -F "\t" '{print $5}' | sort -u > ProstatecaT2d.loci

grep 't2d' $1 | grep 'AlzD' | awk -F "\t" '{print $5}' | sort -u > T2dAlzD.loci

egrep 'plt|pct|pdw|mpv' $1 | grep 'Worry' | awk -F "\t" '{print $5}' | sort -u > PltWorry.loci

grep 'pct' $1 | grep 'Migraine' | awk -F "\t" '{print $5}' | sort -u > PctMigraine.loci

egrep 'SCZ|bipolar' $1 | grep 'TesticCa' | awk -F "\t" '{print $5}' | sort -u > SczBipolarTesticCa.loci

grep 'SCZ' $1 | egrep 'plt|pct|pdw|mpv' | awk -F "\t" '{print $5}' | sort -u > SczPlatelet.loci

grep 'bipolar' $1 | egrep 'plt|pct|pdw|mpv' | awk -F "\t" '{print $5}' | sort -u > BipolarPlatelet.loci

grep 'eo_p' $1 | grep 'Eczema' | awk -F "\t" '{print $5}' | sort -u > EoEczema.loci

grep 'SCZ' $1 | grep 'bipolar' | awk -F "\t" '{print $5}' | sort -u > SczBipolar.loci

grep 'baso' $1 | egrep 'Worry|DepAffect' | awk -F "\t" '{print $5}' | sort -u > BasoAnxEhlersDanlos.loci

grep 'TC' $1 | grep 'ret' | awk -F "\t" '{print $5}' | sort -u > TCret.HFE.loci

grep 'TC' $1 | grep 'plt' | awk -F "\t" '{print $5}' | sort -u > TCplt.TRAIL.loci

grep 'wbc' $1 | grep 'crohns' | awk -F "\t" '{print $5}' | sort -u > CrohnsWBC.PTPN22.loci

grep 'rbc' $1 | grep 'DepSx' | awk -F "\t" '{print $5}' | sort -u > RBC.DepSx.loci
grep 'baso' $1 |grep 'DepSx' | awk -F "\t" '{print $5}' | sort -u > Baso.DepSx.loci
grep 'rbc' $1 | grep 'baso' | grep 'DepSx' | awk -F "\t" '{print $5}' | sort -u > RBC.Baso.DepSx.loci

grep 'lymph_p' $1 | grep 'AlzD' | awk -F "\t" '{print $5}' | sort -u > Lymph_p.AlzD.loci
grep 'T2D' $1 | grep 'AlzD'| awk -F "\t" '{print $5}' | sort -u > T2D.AlzD.loci

egrep 'rdw|mch|mcv' $1 | awk -F "\t" '{print $5}' | sort -u > rdw.mch.mcv.loci
egrep 'mpv|plt|pdw' $1 | awk -F "\t" '{print $5}' | sort -u > mpv.plt.pdw.loci
grep 'SCZ' $1 | grep 'BrCaC' | awk -F "\t" '{print $5}' | sort -u > BrCaC.Scz.loci


=====================================================
=====================================================


#WordHeatMap.py by _CST_ 190702
#!usr/bin/python

from io import StringIO
import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import CountVectorizer
import math
import sys
import subprocess

if len(sys.argv) != 3 :
    print("\nUsage: python WordHeatMap.py <Input file containing ONLY space-separated trait words> <Outfile with a row-normalized matrix with % colocalization>\n\nRemember to rm tmp.csv if you are doing this multiple times!!\n\
n")
    exit()
    
traits = sys.argv[1]
outfile = sys.argv[2]

#clean slate each time this runs
subprocess.call("rm TraitsPerLine", shell=True) #so that it doesn't keep appending to same file each time when this is rerun
subprocess.call("rm words.tmp.csv", shell=True)

for f in open(traits, 'r'):
    f = f.replace("\n","").split(" ")
    ar = np.unique(f)
    
    ########### this prints out number of unique traits on each line to make histogram later
    tn = len(ar)
    out = open(r'TraitsPerLine', 'a')
    out.write(str(tn)+"\n")
    out.close()
    ######################
    
    df = pd.DataFrame(ar)
    df = df.T #transposed to 1 row
    df.to_csv("words.tmp.csv", mode='a', sep=" ", header=None, index=None)
    
with open("words.tmp.csv", 'r') as f:
    vectorizer = CountVectorizer()
    df = vectorizer.fit_transform(f).toarray()
    print(vectorizer.vocabulary_)
    print(vectorizer.get_feature_names())
    print(df.shape)
    df_asint = df.astype(int)
    coocc = df_asint.T.dot(df_asint)
    print(coocc)
    print(coocc.shape)
    column_names = vectorizer.get_feature_names()
    row_names = vectorizer.get_feature_names()
    df = pd.DataFrame(coocc, columns=column_names, index=row_names)
    
    #Heme Traits reorganized for PPFC>0.7
    df = df.reindex(['rdw', 'mchc', 'mcv', 'mch', 'pdw', 'mpv', 'pct', 'plt', 'hgb', 'hct', 'rbc', 'ret_p', 'ret', 'hlr_p', 'hlr', 'irf', 'baso_p_gran', 'baso_p', 'baso', 'eo_p_gran', 'eo_p', 'neut_p_gran', 'neut_p', 'lymph_p'
, 'lymph', 'mono_p', 'gran_p_myeloid_wbc', 'mono', 'gran', 'neut_eo_sum', 'baso_neut_sum', 'neut', 'myeloid_wbc', 'wbc'])

    #Heme reorganized for PPFC > 0.7
    columnTitles = ['rdw', 'mchc', 'mcv', 'mch', 'pdw', 'mpv', 'pct', 'plt', 'hgb', 'hct', 'rbc', 'ret_p', 'ret', 'hlr_p', 'hlr', 'irf', 'baso_p_gran', 'baso_p', 'baso', 'eo_p_gran', 'eo_p', 'neut_p_gran', 'neut_p', 'lymph_p',
 'lymph', 'mono_p', 'gran_p_myeloid_wbc', 'mono', 'gran', 'neut_eo_sum', 'baso_neut_sum', 'neut', 'myeloid_wbc', 'wbc']
 
    df = df.reindex(columns=columnTitles)
    dfMax = df.max(axis=1)
    df = df.divide(dfMax, axis=0)
    df.to_csv(path_or_buf=outfile, sep="\t")


##########################################################################
# Fig. 1A

library("ggplot2")
library(gplots)
library("reshape2")
library(RColorBrewer)
library("ape")

#Identify data
setwd("/project/voight_ML/thomc/thomc_results/HyPrCOLOC/190822.AstleTraits.HyPrColoc")
d <- read.table(file="ColocMatrix.190822.Astle.ChrPos.minPPFC0.7.txt", header=TRUE)


# heat map - https://stat.ethz.ch/R-manual/R-patched/library/stats/html/heatmap.html
# transpose and make symmetrical : https://davetang.org/muse/2014/01/22/making-symmetric-matrices-in-r/
m <- data.matrix(d) # labRow=rownames(d), labCol=colnames(d)) #Rowv=dendrogram
# <- t(lower.tri(m))
m[upper.tri(m)] <- NA 
u <- t(m)
m[upper.tri(m)] <- u[upper.tri(u)]
print(dim(m))

wbc <- c('wbc', 'lymph', 'lymph_p', 'myeloid_wbc', 'neut', 'neut_p', 'neut_p_gran', 'neut_eo_sum', 'baso_neut_sum', 'baso', 'baso_p', 'baso_p_gran', 'eo_p', 'eo_p_gran', 'gran', 'gran_p_myeloid_wbc', 'mono', 'mono_p')
plt <- c('plt', 'pct', 'mpv', 'pdw')
red <- c('rbc', 'hct', 'hgb', 'mch', 'mchc', 'mcv', 'hlr', 'hlr_p', 'irf', 'rdw', 'ret', 'ret_p')

wbc_col=sample(c("gray"), length(wbc), replace = TRUE, prob = NULL)
plt_col=sample(c("purple"), length(plt), replace = TRUE, prob = NULL)
red_col=sample(c("red"), length(red), replace = TRUE, prob = NULL)

# separate cluster and create dendrogram
clust <- dist(as.matrix(d)) 
hc <- hclust(clust, method = "complete" )
dd <- as.dendrogram(hc)
pdf("clusteringtrial.pdf", height=10, width=5)
par(cex=0.7, mar=c(10,10,10,10))
#reorder
wts = matrix(nrow = 1, ncol = nrow(m))
wts[1,] = 1
wts[1,1] = 1000
wts[1,6] = 900
wts[1,19] = 200
wts[1,31] = 1
plot(reorder(dd, wts, agglo.FUN = mean), horiz=T, lwd = 2, main = "reordered")
dev.off()

#unrooted
pdf("unrooted.pdf", width=10, height=10)
phylo <- plot(as.phylo(hc), type = "fan", cex =1.5, tip.color = c(wbc_col,plt_col,red_col,cm_col,ai_col,psych_col,can_col)) #no.margin=TRUE
dev.off()

####################
#plots
#for heme traits

# For TraitsPerLocus histogram
p <- read.table(file="TraitsPerLine", header=F)
pdf("TraitsPerLocus.pdf", height=10, width=5)
ggplot(p, aes(x=factor(p$V1))) + geom_bar(fill='black') + theme_classic(base_size=24) + scale_x_discrete(limits=1:25, breaks=c(1,5,10,15,20,25)) + scale_y_continuous(expand = c(0, 0)) #, limits = c(0, 1000), breaks=c(0, 250, 50
0, 750, 1000))
dev.off()



##########################################################################
# Fig. 1B (continuation of above)


pdf("190822.AstleTraits.minPPFC0.7.wCluster.pdf")
dd <- as.dendrogram(reorder(dd, wts, agglo.FUN = mean), horiz=T)
heatmap <- heatmap.2(m, dendrogram="none", Rowv=dd, Colv=rev(dd), col=brewer.pal(9, "YlOrRd"), scale="none", margins=c(5,5), ColSideColors=matrix(c(wbc_col,plt_col,red_col), nrow=1, ncol=ncol(m)), RowSideColors=matrix(c(wbc_col
,plt_col,red_col), nrow=nrow(m), ncol=1), cexRow=1, offsetRow=0, offsetCol=0, trace='none', density.info=c("none"), key=FALSE) 
dev.off()

pdf("190822.AstleTraits.minPPFC0.7.pdf")
heatmap <- heatmap.2(m, dendrogram="none", Rowv=FALSE, Colv=FALSE, col=brewer.pal(9, "YlOrRd"), scale="none", margins=c(10,10), ColSideColors=matrix(c(wbc_col,plt_col,red_col), nrow=1, ncol=ncol(m)), RowSideColors=matrix(c(wbc_
col,plt_col,red_col), nrow=nrow(m), ncol=1), cexRow=0.5, cexCol=1, srtCol=90, offsetRow=-45, offsetCol=0, trace='none', density.info=c("none"), key=FALSE, symkey=FALSE, keysize=1, key.title="Overlap %", key.xlab="Overlap %") #,
 RowSideColorsSize=2, ColSideColorsSize=2) #, lmat = lmat, lwid = lwid, lhei = lhei) 
dev.off()

pdf("legend.pdf")
heatmap <- heatmap.2(m, dendrogram="none", Rowv=FALSE, Colv=FALSE, col=brewer.pal(9, "YlOrRd"), scale="none", margins=c(1,1), ColSideColors=matrix(c(wbc_col,plt_col,red_col), nrow=1, ncol=ncol(m)), RowSideColors=matrix(c(wbc_co
l,plt_col,red_col), nrow=nrow(m), ncol=1), cexRow=0.5, cexCol=1, srtCol=90, offsetRow=-45, offsetCol=0, trace='none', density.info=c("none"), key=TRUE, symkey=FALSE, keysize=2, key.title="Overlap %", key.xlab="Overlap %") #, Ro
wSideColorsSize=2, ColSideColorsSize=2) #, lmat = lmat, lwid = lwid, lhei = lhei) 

legend('topright', legend=c("White cell","Platelet","Red cell"), fill=c("gray","purple","red"), border=FALSE, bty="n", y.intersp = 0.8, cex=1)
dev.off()




##########################################################################
# Fig. 1C: Calculate LDSC scores, then coloc # assemble into xls and analyze by Prism

#LDSC.R
library(data.table)
library(dplyr)
library(GenomicRanges)
#library(BuenColors)
#library(diffloop)
options(datatable.fread.input.cmd.message=FALSE) #suppresses warning message associated with fread (see https://cran.r-project.org/web/packages/data.table/news/news.html for details)

setwd<-("/project/voight_datasets/annot/LDSC/")

# Import LD Scores 
ldscorefiles <- list.files("/project/voight_datasets/annot/LDSC/eur_w_ld_chr", pattern = "*.gz$", full.names = TRUE)
print(ldscorefiles)

ldsc_g <- lapply(ldscorefiles, function(file){
  dt <- fread(paste0("zcat < ",file))
  dt 
}) %>% rbindlist() %>% as.data.frame() %>%
  makeGRangesFromDataFrame(keep.extra.columns = TRUE, seqnames.field = "CHR", start.field = "BP", end.field = "BP")


# Import files
narrowpeakfiles <- list.files("/project/voight_datasets/GWAS/12_haemgen/Astle_2016/", pattern = "*_ukbb_ukbil_meta.tsv.gz$", full.names = TRUE)
list_np_g <- lapply(narrowpeakfiles, function(file){
	  dg <- makeGRangesFromDataFrame(data.frame(fread(paste0("zcat < ",file), header = FALSE)), 
                                 seqnames.field = "CHR", start.field = "POS", end.field = "POS") %>% rmchr()
  dg 
})

names(list_np_g) <- gsub("_ukbb_ukbil_meta.tsv.gz", "", list.files("/project/voight_datasets/GWAS/12_haemgen/Astle_2016/", pattern = "*_ukbb_ukbil_meta.tsv.gz$"))

# Extract cell-type specific LD scores
lapply(1:length(list_np_g), function(i){
  peaks <- list_np_g[[i]]
  snpsidx <- findOverlaps(peaks, ldsc_g) %>% subjectHits() %>% unique()
  ldscs <- mcols(ldsc_g)$L2[snpsidx]
  df <- data.frame(celltype = names(list_np_g)[i], scores = ldscs)
  return(df)
}) %>% rbindlist() %>% as.data.frame() -> SNPs_LDscores_celltypes

p1 <- ggplot(SNPs_LDscores_celltypes, aes(x = celltype, y = scores, fill = celltype)) +
  geom_boxplot(outlier.colour=NA) + scale_fill_manual(values = ejc_color_maps) +
  pretty_plot() +
    theme(plot.subtitle = element_text(vjust = 1), 
    plot.caption = element_text(vjust = 1), legend.position = "none",
    axis.text.x = element_text(angle = 90)) +labs(x = NULL, y = "Variant-Level LD Scores", 
    fill = "Cell Type") + scale_y_continuous(limits = c(-3, 80))
ggsave(p1, file = "variantLevelLDscores.pdf")

anova(lm(scores ~ as.factor(celltype), SNPs_LDscores_celltypes))



###########
#submitLDSC.py
#!usr/bin/python
import sys
import subprocess
import glob
import pdb
from itertools import combinations
import os
import pandas as pd
import numpy as np
import time

if len(sys.argv) != 2 :
        print("\n\nUsage: submitLDSC.py: python </path/to/submitLDSC.py> </path/to/TraitsDirectories, which is trait -t- directory>\n\nMight need to set environment? Look in Benchling LDSC\n\n")
        exit()

traits = sys.argv[1]


try:
        with open(traits, 'r') as td: 
                
                #Munge first, then run LDScore Regression
                #for file in glob.glob(traits):
                for line in td:
                        sl = line.split('\t')
                        file = sl[1].rstrip('\n')
                        trait = sl[0]
                        #print(file)
                        #pdb.set_trace()
                        if os.path.isfile(trait+".Munged.sumstats.gz"):
                                print(trait+".Munged.sumstats.gz already exists")
                                
                        if not os.path.isfile(trait+".Munged.sumstats.gz"):
                                base = "python /project/voight_ML/thomc/thomc_results/LDSC/ldsc/munge_sumstats.py --sumstats "+file +"  --merge-alleles /project/voight_selscan/ksiewert/Migraine_Project/LDScore/LDScoreData/w_hm3
.snplist --out " + trait +".Munged"
                                #print(base)
                                subprocess.call(base, shell=True)
                td.close()
except IOError:
        print("cannot open traits file")
        exit()



try:
        with open(traits, 'r') as td: 
                for (file1, file2) in combinations(td, 2):
                        #print(file1+ file2)
                        f1 = file1.split('\t')
                        f2 = file2.split('\t')
                        f1t = f1[0]
                        f1d = f1[1]
                        f2t = f2[0]
                        f2d = f2[1]
                        #print(f1t+" , "+f2t)
                        #for file in glob.glob("./*Munged.sumstats.gz"):
                        #Ctype = file.split(".")[1].replace("Munged","").replace("/","")
                        if not os.path.isfile(f1t+"."+f2t+".ldscresults.log"):
                                base = "bsub -o LDSCreg.out -e LDSCreg.err python /project/voight_ML/thomc/thomc_results/LDSC/ldsc/ldsc.py --rg " +f1t+ ".Munged.sumstats.gz," +f2t+ ".Munged.sumstats.gz --ref-ld-chr /project/voi
ght_ML/thomc/thomc_results/LDSC/LDSC.Tutorial/eur_w_ld_chr/ --w-ld-chr /project/voight_ML/thomc/thomc_results/LDSC/LDSC.Tutorial/eur_w_ld_chr/ --out " +f1t+"."+f2t+".ldscresults"
                                #print(base)
                                subprocess.call(base, shell=True)
                        
except IOError:
        print("cannot open traits file named: " + file1 + "  " + file2)
        exit()


#make.rg.dataframe.sh (END)
awk '/^p1/ {getline; print $1"\t"$2"\t"$3}' *.ldscresults.log > LDSCresults
#awk '/^p1/ {getline; print $1"\t"$2"\t"$3}' LDSCreg.out > LDSCresults
sed -i 's/\.Munged\.sumstats\.gz//g' LDSCresults

